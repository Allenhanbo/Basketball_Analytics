{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data will be from 1980-81 up to past season for train and validation splits. That is because prior to that season voting was done by players."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def work_player_profile(param, season):\n",
    "    url = \"https://www.basketball-reference.com\" + param\n",
    "    res = requests.get(url)\n",
    "    soup = BeautifulSoup(res.text)\n",
    "    \n",
    "    data_dict = {}\n",
    "    \n",
    "    per_game = soup.find(attrs={'id': 'all_per_game'})\n",
    "    for row in per_game.findAll(\"tr\"):\n",
    "        if 'id' in row.attrs and row.attrs['id'] == \"per_game.\" + season:\n",
    "            data_dict['fga'] = float(row.find('td', attrs={'data-stat': 'fga_per_g'}).text)\n",
    "            data_dict['fg3a'] = float(row.find('td', attrs={'data-stat': 'fg3a_per_g'}).text)\n",
    "            data_dict['fta'] = float(row.find('td', attrs={'data-stat': 'fta_per_g'}).text)\n",
    "            break\n",
    "    \n",
    "    advanced_table = soup.find(attrs={'id': 'all_advanced'})\n",
    "    for child in advanced_table.children:\n",
    "        if \"table_outer_container\" in child:\n",
    "            other_soup = BeautifulSoup(child)\n",
    "            rows = other_soup.findAll(\"tr\")\n",
    "    for row in rows:\n",
    "        if 'id' in row.attrs and row.attrs['id'] == \"advanced.\" + season:\n",
    "            data_dict.update(\n",
    "                {\n",
    "                    'per': float(row.find('td', attrs={'data-stat': 'per'}).text),\n",
    "                    'ts_pct': float(row.find('td', attrs={'data-stat': 'ts_pct'}).text),\n",
    "                    'usg_pct': float(row.find('td', attrs={'data-stat': 'usg_pct'}).text),\n",
    "                    'bpm': float(row.find('td', attrs={'data-stat': 'bpm'}).text),\n",
    "                    'season': str(int(season)-1) + \"-\" + season[-2:],\n",
    "                }\n",
    "            )\n",
    "            return data_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats_of_voting(url):\n",
    "    res = requests.get(url)\n",
    "    soup = BeautifulSoup(res.text)\n",
    "    item = soup.find(attrs={'class': 'stats_table'})\n",
    "    rows = item.findAll(\"tr\")\n",
    "    \n",
    "    season = url.split(\".html\")[0][-4:]\n",
    "    \n",
    "    print(f\"Current season: {season}\")\n",
    "    \n",
    "    players_stats = defaultdict(list)\n",
    "    \n",
    "    for index, row in enumerate(rows):\n",
    "        \n",
    "        header_cells = row.findAll(\"th\")\n",
    "        for header_cell in header_cells:\n",
    "            if 'data-stat' in header_cell.attrs and header_cell['data-stat'] == 'ranker' and 'csk' in header_cell.attrs:\n",
    "                rank = int(header_cell.getText())\n",
    "        td_cells = row.findAll(\"td\")\n",
    "        if not td_cells:\n",
    "            continue\n",
    "        for cell in td_cells:\n",
    "            if 'data-stat' not in cell.attrs:\n",
    "                continue\n",
    "            if cell['data-stat'] == 'age' or cell['data-stat'] == 'team_id':\n",
    "                continue\n",
    "            if cell['data-stat'] == 'player':\n",
    "                time.sleep(1)\n",
    "                advanced_dict = work_player_profile(cell.find(\"a\")['href'], season)\n",
    "                for key in advanced_dict:\n",
    "                    players_stats[key].append(advanced_dict[key])\n",
    "                players_stats[cell['data-stat']].append(cell.getText())\n",
    "            else:\n",
    "                text = cell.getText() or \"0\"\n",
    "                players_stats[cell['data-stat']].append(float(text))\n",
    "    return players_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current season: 1981\n",
      "Current season: 1982\n",
      "Current season: 1983\n",
      "Current season: 1984\n",
      "Current season: 1985\n",
      "Current season: 1986\n",
      "Current season: 1987\n",
      "Current season: 1988\n",
      "Current season: 1989\n",
      "Current season: 1990\n",
      "Current season: 1991\n",
      "Current season: 1992\n",
      "Current season: 1993\n",
      "Current season: 1994\n",
      "Current season: 1995\n",
      "Current season: 1996\n",
      "Current season: 1997\n",
      "Current season: 1998\n",
      "Current season: 1999\n",
      "Current season: 2000\n",
      "Current season: 2001\n",
      "Current season: 2002\n",
      "Current season: 2003\n",
      "Current season: 2004\n",
      "Current season: 2005\n",
      "Current season: 2006\n",
      "Current season: 2007\n",
      "Current season: 2008\n",
      "Current season: 2009\n",
      "Current season: 2010\n",
      "Current season: 2011\n",
      "Current season: 2012\n",
      "Current season: 2013\n",
      "Current season: 2014\n",
      "Current season: 2015\n",
      "Current season: 2016\n",
      "Current season: 2017\n",
      "Current season: 2018\n"
     ]
    }
   ],
   "source": [
    "seasons = range(1981, 2019)\n",
    "\n",
    "data_mvp = defaultdict(list)\n",
    "\n",
    "for season in seasons:\n",
    "    full_url = f\"https://www.basketball-reference.com/awards/awards_{str(season)}.html\"\n",
    "    cur_season_dict = get_stats_of_voting(full_url)\n",
    "    for key in cur_season_dict:\n",
    "        data_mvp[key].extend(cur_season_dict[key])\n",
    "        \n",
    "data_frame = pd.DataFrame(data_mvp)\n",
    "data_frame.to_csv(\"../../Data/mvp_data/mvp_votings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = pd.read_csv(\"../../../Data/mvp_data/mvp_votings.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures, normalize, StandardScaler, MinMaxScaler\n",
    "from sklearn.linear_model import SGDRegressor, LinearRegression, Ridge, Lasso\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(data_frame, estimators, params, should_scale, should_poly):\n",
    "    \n",
    "    unique_seasons = data_frame.season.unique()\n",
    "    \n",
    "    train_columns = [\n",
    "        'fga', 'fg3a', 'fta', 'fg_pct', 'fg3_pct', 'ft_pct', 'per', 'ts_pct', 'usg_pct', 'bpm',\n",
    "        'mp_per_g', 'pts_per_g', 'trb_per_g', 'ast_per_g', 'stl_per_g', 'blk_per_g', 'ws_per_48'            \n",
    "    ]\n",
    "    target_columns = ['award_share']\n",
    "    \n",
    "    for handler in logging.root.handlers[:]:\n",
    "        logging.root.removeHandler(handler)\n",
    "    logging.basicConfig(filename='log_no_scale_poly_2_no_inter.txt', filemode='w', level=logging.INFO)\n",
    "    logger = logging.getLogger()\n",
    "    \n",
    "    minimal_error, best_estimator = None, None\n",
    "    \n",
    "    for estimator in estimators:\n",
    "        \n",
    "        print(f\"Starting with estimator: {estimator.__name__}\")\n",
    "        logging.info(f\"Starting with estimator: {estimator.__name__}\")\n",
    "        \n",
    "        for index, cur_params in enumerate(params[estimator.__name__]):\n",
    "            \n",
    "            regressor = estimator(**cur_params)\n",
    "            \n",
    "            # To collect MSE over each split\n",
    "            errors = []\n",
    "    \n",
    "            for season in unique_seasons:\n",
    "\n",
    "                train_data = data_frame.loc[data_frame.season != season]\n",
    "                validation_data = data_frame.loc[data_frame.season == season]\n",
    "\n",
    "                # Get train data\n",
    "                train_x = train_data[train_columns].to_numpy()\n",
    "                train_y = train_data[target_columns].to_numpy()\n",
    "                train_y = train_y.reshape(train_y.shape[0], )\n",
    "\n",
    "                # Validate over one season only\n",
    "                val_x = validation_data[train_columns].to_numpy()\n",
    "                val_y = validation_data[target_columns].to_numpy()\n",
    "                val_y = val_y.reshape(val_y.shape[0], )\n",
    "                \n",
    "                if should_poly:\n",
    "                    poly_fit = PolynomialFeatures(degree=2, interaction_only=False)\n",
    "                    train_x = poly_fit.fit_transform(train_x)\n",
    "                    val_x = poly_fit.fit_transform(val_x)\n",
    "                    \n",
    "                if should_scale:\n",
    "                    min_max_scaler = MinMaxScaler()\n",
    "                    train_x = min_max_scaler.fit_transform(train_x)\n",
    "                    val_x = min_max_scaler.fit_transform(val_x)\n",
    "                \n",
    "                shuffle_x, shuffle_y = shuffle(train_x, train_y)\n",
    "                \n",
    "                regressor.fit(shuffle_x, shuffle_y)\n",
    "                predicted_y = regressor.predict(val_x)\n",
    "                \n",
    "                curr_error = mean_squared_error(y_true=val_y, y_pred=predicted_y)\n",
    "                errors.append(curr_error)\n",
    "            \n",
    "            mean_error = np.average(errors)\n",
    "            logging.info(f\"Params: {cur_params}, MSE over all splits is: {mean_error:.4f}, with scale: {should_scale}\")\n",
    "            print(f\"Params: {cur_params}, MSE over all splits is: {mean_error:.4f}, with scale: {should_scale}\")\n",
    "            \n",
    "            if minimal_error is None or mean_error < minimal_error:\n",
    "                minimal_error = mean_error\n",
    "                best_estimator = estimator(*cur_params)\n",
    "            \n",
    "    return best_estimator\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_rbf_svr():\n",
    "    dicts = []\n",
    "    for C in [0.1, 1, 10, 50, 100]:\n",
    "        for gamma in [0.00001, 0.0001, 0.001, 0.01, 0.1]:\n",
    "            dicts.append(\n",
    "                {\n",
    "                    'kernel': 'rbf',\n",
    "                    'C': C,\n",
    "                    'gamma': gamma\n",
    "                }\n",
    "            )\n",
    "    return dicts\n",
    "\n",
    "def define_poly_svr():\n",
    "    dicts = []\n",
    "    for C in [0.1, 1, 10, 100]:\n",
    "        for gamma in [0.0001, 0.001, 0.01, 0.1]:\n",
    "            for degree in [1, 2, 3]:\n",
    "                dicts.append(\n",
    "                    {\n",
    "                        'kernel': 'poly',\n",
    "                        'C': C,\n",
    "                        'gamma': gamma,\n",
    "                        'degree': degree\n",
    "                    }\n",
    "                )\n",
    "    return dicts\n",
    "\n",
    "def define_linear_svr():\n",
    "    dicts = []\n",
    "    for C in [0.1, 1, 10, 100]:\n",
    "        dicts.append(\n",
    "            {\n",
    "                'kernel': 'linear',\n",
    "                'C': C\n",
    "            }\n",
    "        )\n",
    "    return dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLinearRegression.__name__: [\\n    True,\\n    True,\\n    False,\\n    False,\\n],\\n'"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    \"\"\"\n",
    "    LinearRegression.__name__: [\n",
    "        {\n",
    "            'n_jobs': -1,\n",
    "        },\n",
    "        {\n",
    "            'n_jobs': -1,\n",
    "            'normalize': True\n",
    "        },\n",
    "        {\n",
    "            'n_jobs': -1,\n",
    "        },\n",
    "        {\n",
    "            'n_jobs': -1,\n",
    "            'normalize': True\n",
    "        }\n",
    "    ],\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    LinearRegression.__name__: [\n",
    "        True,\n",
    "        True,\n",
    "        False,\n",
    "        False,\n",
    "    ],\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [\n",
    "    LinearRegression,\n",
    "    Ridge,\n",
    "    Lasso,\n",
    "    #SVR,\n",
    "]\n",
    "params = {\n",
    "    LinearRegression.__name__: [\n",
    "        {\n",
    "            'n_jobs': -1,\n",
    "        },\n",
    "        {\n",
    "            'n_jobs': -1,\n",
    "            'normalize': True\n",
    "        },\n",
    "    ],\n",
    "    Ridge.__name__: [\n",
    "        {\n",
    "            'alpha': 1.0\n",
    "        },\n",
    "        {\n",
    "            'alpha': 10.0\n",
    "        },\n",
    "        {\n",
    "            'alpha': 50.0\n",
    "        },\n",
    "        {\n",
    "            'alpha': 100.0\n",
    "        },\n",
    "        {\n",
    "            'alpha': 200.0\n",
    "        },\n",
    "        {\n",
    "            'alpha': 500.0\n",
    "        },\n",
    "    ],\n",
    "    Lasso.__name__: [\n",
    "        {\n",
    "            'alpha': 1.0,\n",
    "        },\n",
    "        {\n",
    "            'alpha': 10.0\n",
    "        },\n",
    "        {\n",
    "            'alpha': 100.0\n",
    "        },\n",
    "    ],\n",
    "    #SVR.__name__: [\n",
    "        #*define_poly_svr(),\n",
    "        #*define_linear_svr(),\n",
    "        #*define_rbf_svr(),\n",
    "    #]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with estimator: LinearRegression\n",
      "Params: {'n_jobs': -1}, MSE over all splits is: 0.0649, with scale: False\n",
      "Params: {'n_jobs': -1, 'normalize': True}, MSE over all splits is: 0.0359, with scale: False\n",
      "Starting with estimator: Ridge\n",
      "Params: {'alpha': 1.0}, MSE over all splits is: 0.0312, with scale: False\n",
      "Params: {'alpha': 10.0}, MSE over all splits is: 0.0309, with scale: False\n",
      "Params: {'alpha': 50.0}, MSE over all splits is: 0.0306, with scale: False\n",
      "Params: {'alpha': 100.0}, MSE over all splits is: 0.0306, with scale: False\n",
      "Params: {'alpha': 200.0}, MSE over all splits is: 0.0310, with scale: False\n",
      "Params: {'alpha': 500.0}, MSE over all splits is: 0.0322, with scale: False\n",
      "Starting with estimator: Lasso\n",
      "Params: {'alpha': 1.0}, MSE over all splits is: 0.0395, with scale: False\n",
      "Params: {'alpha': 10.0}, MSE over all splits is: 0.0462, with scale: False\n",
      "Params: {'alpha': 100.0}, MSE over all splits is: 0.0693, with scale: False\n"
     ]
    }
   ],
   "source": [
    "best_estimator = pipeline(\n",
    "    data_frame=data_frame,\n",
    "    estimators=estimators,\n",
    "    params=params,\n",
    "    should_scale=False,\n",
    "    should_poly=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results of couple of simple regressors, the best one is Ridge regressor with poly level 2 and all polynoms (not only interacitons). Alpha is set to 50 or 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_columns = [\n",
    "    'fga', 'fg3a', 'fta', 'fg_pct', 'fg3_pct', 'ft_pct', 'per', 'ts_pct', 'usg_pct', 'bpm',\n",
    "    'mp_per_g', 'pts_per_g', 'trb_per_g', 'ast_per_g', 'stl_per_g', 'blk_per_g', 'ws_per_48'            \n",
    "]\n",
    "target_columns = ['award_share']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_columns = [\n",
    "    'FGA', '3PA', 'FTA', 'FG_pct', '3P_pct', 'FT_pct', 'PER', 'TS_pct', 'USG_pct', 'BPM',\n",
    "    'MP_x', 'PTS', 'TRB', 'AST', 'STL', 'BLK', 'WS_48'            \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_g_df = pd.read_csv('player_per_g.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "advanced_df = pd.read_csv('advanced.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(per_g_df, advanced_df, on=['PlayerId', 'Player'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_merged = merged.sort_values(by='PER', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_players = sorted_merged[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = top_players[test_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using top model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = data_frame[train_columns].to_numpy()\n",
    "train_y = data_frame[target_columns].to_numpy()\n",
    "\n",
    "shuffle_x, shuffle_y = shuffle(train_x, train_y)\n",
    "train_y = train_y.reshape(train_y.shape[0], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge(alpha=100)\n",
    "poly_fit = PolynomialFeatures(degree=2, interaction_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = poly_fit.fit_transform(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=100, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = test_x.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = np.nan_to_num(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = poly_fit.fit_transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_y = ridge.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_indices = np.argsort(predict_y)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict_y[sorted_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Giannis Antetokounmpo: 0.8411406251629865\n",
      "2. James Harden: 0.5289575080559985\n",
      "3. Nikola Jokic: 0.37276804561100846\n",
      "4. Rudy Gobert: 0.2273300234762307\n",
      "5. Kawhi Leonard: 0.18433245210820604\n",
      "6. Kyrie Irving: 0.16419885310873972\n",
      "7. Kevin Durant: 0.11770066232962462\n",
      "8. Clint Capela: 0.10063022152486134\n",
      "9. Damian Lillard: 0.09804011426326209\n",
      "10. Russell Westbrook: 0.08824654886066852\n",
      "11. Nikola Vucevic: 0.06581035736472052\n",
      "12. Ben Simmons: 0.05633753587527501\n",
      "13. Chris Paul: 0.047065930067474615\n",
      "14. Joel Embiid: 0.03218541730778879\n"
     ]
    }
   ],
   "source": [
    "rank = 1\n",
    "for i in range(len(sorted_indices)):\n",
    "    if predictions[i] < 0:\n",
    "        continue\n",
    "    print(f\"{rank}. {top_players.iloc[sorted_indices[i]].Player}: {predictions[i]}\")\n",
    "    rank += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
